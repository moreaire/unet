{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e7e0515",
   "metadata": {},
   "source": [
    "## Save Preprocessed Data\n",
    "\n",
    "\n",
    "This notebook preprocesses and applies random crops to the data before saving each image. \n",
    "\n",
    "TODO: Add original image name to the .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bcda1352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import skimage.io\n",
    "import torch\n",
    "from torchvision.transforms import ToTensor, Lambda, RandomCrop\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.ndimage.measurements import center_of_mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "859e4691",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOR PREPROCESSING ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3db2f5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_rand_seed(seed=10):\n",
    "    '''\n",
    "    Creates random seed for each library so that randomness is repeatable. Initialized first to set all randomness\n",
    "    '''\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "def save(sample, mask, i, sample_name, sample_address, cropped):\n",
    "    \"\"\"\n",
    "    Saves file in sim_data folder with iterating number to correspond to number of samples desired as pytorch tensors\n",
    "    This file can deal with either cropped or original images, and it will sort for each one.\n",
    "    In our case, cropped means that it had the n number of random crops, and they are all saved\n",
    "    in a list together. \n",
    "    \"\"\"\n",
    "\n",
    "    if cropped:\n",
    "        #All randomly cropped images will come in a list. This parses through the list and \n",
    "        #deals with each new image\n",
    "        for j in range (len(sample)):\n",
    "            \n",
    "            #Converting to tensors\n",
    "            input_tensor = torch.tensor(sample[j])\n",
    "            mask_tensor = torch.tensor(mask[j])\n",
    "            \n",
    "            #Saving each image\n",
    "            file_name = f\"/nsls2/users/maire1/unet/data/preprocessed_data/img{i}.pt\"\n",
    "            torch.save({\"input\": input_tensor, \"target\": mask_tensor}, file_name)\n",
    "            \n",
    "            #Adding each name and address to be ultimately saved in .csv file\n",
    "            sample_name.append(f\"img{i}.pt\")\n",
    "            sample_address.append(file_name)\n",
    "\n",
    "    else:\n",
    "        #Convert to tensor\n",
    "        input_tensor = torch.Tensor(sample)\n",
    "        mask_tensor = torch.Tensor(mask)\n",
    "        \n",
    "        #Saving each image\n",
    "        file_name = f\"/nsls2/users/maire1/unet/data/preprocessed_data/img{i}.pt\"\n",
    "        torch.save({\"input\": input_tensor, \"target\": mask_tensor}, file_name)\n",
    "        \n",
    "        #Adding each name and address to be ultimately saved in .csv file\n",
    "        sample_name.append(f\"img{i}.pt\")\n",
    "        sample_address.append(file_name)\n",
    "\n",
    "    return(sample_name, sample_address)\n",
    "\n",
    "def save_total_data(name, address, cropped):\n",
    "    \"\"\"\n",
    "    Saving titles and sample addresses into a separate csv file for use in the neural network.\n",
    "    \"\"\"\n",
    "\n",
    "    #All randomly cropped images will come in a list. This parses through the list and \n",
    "    #deals with each new image\n",
    "    d = {'sample': name, 'address': address}\n",
    "    filename = '/nsls2/users/maire1/unet/data/preprocessed_data/img_address.csv'\n",
    "\n",
    "    #Convert to pandas dataframe and save\n",
    "    df = pd.DataFrame(data=d)\n",
    "    df.to_csv(filename, index = False)\n",
    "    print(\"All samples completed. Data saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8bc2449e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(orig_img, target, sample_num):\n",
    "    '''\n",
    "    Applies random crop to the original image according to a set size\n",
    "    Outputs: crops and mask crops\n",
    "    '''\n",
    "    set_rand_seed()\n",
    "    \n",
    "    #convert to tensors\n",
    "    pt_img = torch.tensor(orig_img)\n",
    "    pt_target = torch.tensor(target)\n",
    "    cropper = T.RandomCrop(size=(256, 256))\n",
    "    \n",
    "    #ERROR HERE - TODO\n",
    "    crops = [cropper(pt_img) for j in range(sample_num)]\n",
    "    \n",
    "    set_rand_seed() #Need to set random seed twice so that the mask and input have the same crop\n",
    "    mask_crops = [cropper(pt_target) for j in range(sample_num)]\n",
    "    #plot(pt_img, crops)\n",
    "    #plot(pt_target, mask_crops)\n",
    "    return crops, mask_crops\n",
    "\n",
    "def plot(orig_img, imgs, with_orig=True, row_title=None, **imshow_kwargs):\n",
    "    '''This function allows for us to be able to view each image'''\n",
    "    if not isinstance(imgs[0], list):\n",
    "        # Make a 2d grid even if there's just 1 row\n",
    "        imgs = [imgs]\n",
    "\n",
    "    num_rows = len(imgs)\n",
    "    num_cols = len(imgs[0]) + with_orig\n",
    "    fig, axs = plt.subplots(nrows=num_rows, ncols=num_cols, squeeze=False)\n",
    "    for row_idx, row in enumerate(imgs):\n",
    "        row = [orig_img] + row if with_orig else row\n",
    "        for col_idx, img in enumerate(row):\n",
    "            ax = axs[row_idx, col_idx]\n",
    "            ax.imshow(np.asarray(img), **imshow_kwargs)\n",
    "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "    if with_orig:\n",
    "        axs[0, 0].set(title='Original image')\n",
    "        axs[0, 0].title.set_size(8)\n",
    "    if row_title is not None:\n",
    "        for row_idx in range(num_rows):\n",
    "            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2d99a2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(x, n):\n",
    "    xr = np.zeros((x.shape[0]//n,x.shape[1]//n ))\n",
    "    for j in range(0, x.shape[0]-n, n):\n",
    "        for i in range(0, x.shape[1]-n, n):\n",
    "            xr[j//n, i//n] = np.nanmean(x[j:j+n, i:i+n])\n",
    "    return xr\n",
    "\n",
    "# I haven't checked it yet. But the idea should be to selec the nearest label\n",
    "# that is not a background\n",
    "def resize_target(x, n): \n",
    "    xr = np.zeros((x.shape[0]//n,x.shape[1]//n ))\n",
    "    for j in range(0, x.shape[0]-n, n):\n",
    "        for i in range(0, x.shape[1]-n, n):\n",
    "            xr[j//n, i//n] = np.nanmax(x[j:j+n, i:i+n])\n",
    "    return xr\n",
    "\n",
    "def transform(img, target):    \n",
    "    #Most of our tansforms should only be applied to the input image, not the target. \n",
    "    #print(img.shape)\n",
    "    if target:\n",
    "        y = resize(img, 2)\n",
    "    else:\n",
    "        y = resize(img, 2)\n",
    "        y_smooth = gaussian_filter(y, sigma=2)\n",
    "        \n",
    "        #ADDED from another place. May need to change order of these\n",
    "        y_log = np.log(y_smooth + 0.5)\n",
    "        y_log = np.nan_to_num(y_log)\n",
    "        y_log = np.where(np.abs(y_log) > 1000, 0, y_log)\n",
    "        min_val = np.min(y_log.flatten())\n",
    "        box_cox, l = stats.boxcox(y_log.flatten()+np.abs(min_val)+1)\n",
    "        \n",
    "        #This may also need to be changed to account for additional preprocessing methods\n",
    "        y = (y - y_smooth.min())/(y_smooth.max() - y_smooth.min())\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "61e43c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-32-de3861909f28>:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_tensor = torch.tensor(sample[j])\n",
      "<ipython-input-32-de3861909f28>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mask_tensor = torch.tensor(mask[j])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "random_ expects 'from' to be less than 'to', but got from=0 >= to=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-003c2ad19c44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m#sample_name, sample_address = save(img, img_mask, j, sample_name, sample_address, False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mcrops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_crops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mcropped_sample_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcropped_sample_address\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_crops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcropped_sample_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcropped_sample_address\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mj\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-926363e6d80c>\u001b[0m in \u001b[0;36mcrop\u001b[0;34m(orig_img, target, sample_num)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpt_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mcropper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomCrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mcrops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcropper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpt_img\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mset_rand_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Need to set random seed twice so that the mask and input have the same crop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmask_crops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcropper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpt_target\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-926363e6d80c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpt_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mcropper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomCrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mcrops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcropper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpt_img\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mset_rand_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Need to set random seed twice so that the mask and input have the same crop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmask_crops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcropper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpt_target\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SULI/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SULI/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m         \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SULI/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mget_params\u001b[0;34m(img, output_size)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtw\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: random_ expects 'from' to be less than 'to', but got from=0 >= to=0"
     ]
    }
   ],
   "source": [
    "#Opening file with list of addresses for data\n",
    "datafile = '/nsls2/users/maire1/unet/data/cropped_data/data_address.csv'\n",
    "file = pd.read_csv(datafile)\n",
    "partition = file['address']# IDs\n",
    "labels = file['sample']# Labels\n",
    "#name = file['original file name']\n",
    "sample_num = 10\n",
    "\n",
    "#For creating csv file\n",
    "#sample_name = []\n",
    "cropped_sample_name = []\n",
    "#sample_address = []\n",
    "cropped_sample_address = []\n",
    "#orig_name = []\n",
    "\n",
    "size = (256,256)\n",
    "j = 0\n",
    "for i in labels:\n",
    "    \n",
    "    data = torch.load(f'/nsls2/users/maire1/unet/data/cropped_data/{i}')\n",
    "    #data = torch.load(i)\n",
    "    img = data['input'].numpy()\n",
    "    img_mask = data['target'].numpy()\n",
    "    img = transform(img, target=False)\n",
    "    img_mask = transform(img_mask, target=True)\n",
    "#     plt.imshow(img)\n",
    "#     plt.show()\n",
    "\n",
    "    #orig_img = img['input']\n",
    "    #target = img['target']\n",
    "    \n",
    "    #sample_name, sample_address = save(img, img_mask, j, sample_name, sample_address, False)\n",
    "    crops, mask_crops = crop(img, img_mask, sample_num)\n",
    "    cropped_sample_name, cropped_sample_address = save(crops, mask_crops, j, cropped_sample_name, cropped_sample_address, True)\n",
    "    j += 1\n",
    "    \n",
    "#save_total_data(sample_name, sample_address, orig_name, False)\n",
    "save_total_data(cropped_sample_name, cropped_sample_address, orig_name, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed7f7be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744fb620",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
