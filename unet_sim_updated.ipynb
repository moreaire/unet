{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809d6932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bba03d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''SETTING UP U-NET NEURAL NETWORK'''\n",
    "\n",
    "class Block(nn.Module):\n",
    "    '''\n",
    "    Block has three layers: two convolution operators (no padding)\n",
    "    and one with ReLU activation\n",
    "    '''\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 5, padding='same')\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 5, padding='same')        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv2(self.relu(self.conv1(x)))\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    '''\n",
    "    Encoder: Maxpooling (used for downsampling) performed between two Block operations\n",
    "    This class includes the block operations already\n",
    "    '''\n",
    "    def __init__(self, chs=(1,64,128,256,512,1024)):\n",
    "        super().__init__()\n",
    "        self.enc_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)])\n",
    "        self.pool       = nn.MaxPool2d(2)\n",
    "    def forward(self, x):\n",
    "        ftrs = []\n",
    "        for block in self.enc_blocks:\n",
    "            x = block(x)\n",
    "            ftrs.append(x)\n",
    "            x = self.pool(x)\n",
    "        return ftrs\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    '''\n",
    "    Decoder: Series of upconvolutions and decoders to expand matrix in order to determine location\n",
    "    '''\n",
    "    def __init__(self, chs=(1024, 512, 256, 128, 64)):\n",
    "        super().__init__()\n",
    "        self.chs         = chs\n",
    "        self.upconvs    = nn.ModuleList([nn.ConvTranspose2d(chs[i], chs[i+1], 2, 2) for i in range(len(chs)-1)])\n",
    "        self.dec_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)]) \n",
    "        \n",
    "    def forward(self, x, encoder_features):\n",
    "        for i in range(len(self.chs)-1):\n",
    "            x        = self.upconvs[i](x)\n",
    "            enc_ftrs = self.crop(encoder_features[i], x)\n",
    "            x        = torch.cat([x, enc_ftrs], dim=1)\n",
    "            x        = self.dec_blocks[i](x)\n",
    "        return x\n",
    "    \n",
    "    def crop(self, enc_ftrs, x):\n",
    "        _, _, H, W = x.shape\n",
    "        enc_ftrs   = transforms.CenterCrop([H, W])(enc_ftrs)\n",
    "        return enc_ftrs\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    '''\n",
    "    Code to set up encoders and decoder layers\n",
    "    '''\n",
    "    def __init__(self, enc_chs=(1,2,4,8,16,32), dec_chs=(32, 16,8, 4, 2), num_class=3, retain_dim=True, out_sz=(1024,1024)):\n",
    "        super().__init__()\n",
    "        self.encoder     = Encoder(enc_chs)\n",
    "        self.decoder     = Decoder(dec_chs)\n",
    "        self.head        = nn.Conv2d(dec_chs[-1], num_class, 1)\n",
    "        #self.retain_dim  = retain_dim\n",
    "\n",
    "    def forward(self, x, out_sz=(256,256)):\n",
    "        enc_ftrs = self.encoder(x)\n",
    "        out      = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:])\n",
    "        out      = self.head(out)\n",
    "        \n",
    "        #if self.retain_dim:\n",
    "            #out = nn.functional.interpolate(out, out_sz)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd9c848",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''SETUP DATASETS'''\n",
    "class Dataset(Dataset):\n",
    "#'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, list_IDs, labels):\n",
    "        'Initialization'\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # In order to handle different indexing for validation and training data\n",
    "        while True:\n",
    "            try:\n",
    "                ID = self.list_IDs[index]\n",
    "                break\n",
    "            except:\n",
    "                index += len(self.list_IDs)\n",
    "        # Load data and get label\n",
    "        X = torch.load(ID)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2409cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''FUNCTIONS'''\n",
    "\n",
    "def cuda_setup():\n",
    "    # CUDA for PyTorch\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"Using {} device\".format(device))\n",
    "    \n",
    "    return(device)\n",
    "\n",
    "def set_rand_seed(num):\n",
    "    '''\n",
    "    Creates random seed for each library so that randomness is repeatable. Initialized first to set all randomness\n",
    "    '''\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "def normalization(x):\n",
    "    '''\n",
    "    Used to normalize sample data so that it is between 0 and 1\n",
    "    '''\n",
    "    max = torch.max(x)\n",
    "    min = torch.min(x)\n",
    "    x_norm = (x-min)/(max-min)\n",
    "    return x_norm\n",
    "\n",
    "# def optimization_plots(x, y):\n",
    "#     # x: epoch number, y: training\n",
    "#     plt.plot(x, y)\n",
    "#     plt.xlabel(\"Epoch Number\")\n",
    "#     plt.ylabel(\"Loss\")\n",
    "#     plt.title(\"Learning Plot\")\n",
    "#     plt.show()\n",
    "\n",
    "def save_params(output_array, orig_array, params, desc, loss, model, optimizer, i):\n",
    "    '''\n",
    "    Saves what I have changed in each iteration\n",
    "    \n",
    "    Inputs:\n",
    "        Long string of output graphs\n",
    "        List of params\n",
    "        Comments of what was changed. \n",
    "    Outputs: \n",
    "        Pytorch file which lists all of these in a dictionary\n",
    "    '''\n",
    "    \n",
    "    output_tensor = torch.Tensor(output_array)\n",
    "    orig_tensor = torch.Tensor(orig_array)\n",
    "    \n",
    "    torch.save({\n",
    "        \"output\": output_tensor, \n",
    "        \"original\": orig_tensor, \n",
    "        \"params\":params, \n",
    "        \"description\": desc, \n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss}, \n",
    "        f\"/nsls2/users/maire1/unet/params_list{i}.pt\")\n",
    "    i += 1\n",
    "    return i\n",
    "\n",
    "def calculate_weights(target):\n",
    "    #t = normalization(target)\n",
    "    total = target.numel()\n",
    "    stop = target.eq(2).sum().item()/2\n",
    "    streaks = target.eq(1).sum().item()\n",
    "    bkgnd = total - streaks - stop\n",
    "    if streaks == 0:\n",
    "        weights = [1/bkgnd, 0, 1/stop]\n",
    "    else:\n",
    "        weights = [1/bkgnd, 1/streaks, 1/stop]\n",
    "    return weights\n",
    "\n",
    "def rand_input(x, **kwargs):\n",
    "    x.to(device)\n",
    "    val = torch.rand(2).to(device)\n",
    "    x = torch.where(x==0.5,val[0],x)\n",
    "    x = torch.where(x==1, val[1],x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446ad2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, optimizer, device, training_graphs, orig_images):\n",
    "    size = len(dataloader)\n",
    "    model.train()\n",
    "    loss_tot = np.zeros(size)\n",
    "    \n",
    "    for batch_num, local_batch in enumerate (dataloader):\n",
    "\n",
    "        # Transfer to GPU\n",
    "        for key, value in local_batch.items():\n",
    "            local_batch[key] = value.to(device)\n",
    "        #in_batch = local_batch['input'].unsqueeze(1)\n",
    "        in_batch = local_batch['target'].unsqueeze(1)\n",
    "        \n",
    "        in_batch_norm = normalization(in_batch)\n",
    "        in_batch_rand = rand_input(in_batch_norm)\n",
    "        out  = model(in_batch_rand)\n",
    "        #out = model(in_batch)\n",
    "        target = local_batch['target']\n",
    "        \n",
    "        #WEIGHTS\n",
    "        weight = calculate_weights(target)\n",
    "        class_weight = torch.FloatTensor(weight).to(device)\n",
    "        loss_fn = nn.CrossEntropyLoss(class_weight)\n",
    "        \n",
    "        #print(target.type(), target.shape, out.type(), out.shape)\n",
    "        \n",
    "        loss = loss_fn(out, target.long())\n",
    "\n",
    "        #GRAPHING\n",
    "        nn_image = out.argmax(1)\n",
    "        nn_image = nn_image[0,:,:].cpu().numpy()\n",
    "        training_graphs.append(nn_image)\n",
    "        \n",
    "        y_graph = in_batch[0,0,:,:].cpu().numpy()\n",
    "        orig_images.append(y_graph)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss = loss.cpu().detach()\n",
    "        \n",
    "        loss, current = loss.item(), batch_num #* len(out)\n",
    "        # print(f\"loss: {loss:>7f}  [{(current+1):>5d}/{size:>5d}]\")\n",
    "        loss_tot[batch_num] = loss\n",
    "        \n",
    "        if torch.cuda.torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "    mean = np.mean(loss_tot)\n",
    "    print(\"Train Mean Loss: \", mean)\n",
    "    \n",
    "\n",
    "    #GRAPHING CTD.\n",
    "    for i in range (len(training_graphs)):\n",
    "        plt.imshow(training_graphs[i])\n",
    "        plt.show()\n",
    "\n",
    "        plt.imshow(orig_images[i])\n",
    "        plt.show()\n",
    "\n",
    "    return training_graphs, orig_images, loss, optimizer\n",
    "    \n",
    "                \n",
    "def validation(dataloader, model, device):\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, local_batch in enumerate (dataloader):\n",
    "            # Transfer to GPU\n",
    "            \n",
    "            for key, value in local_batch.items():\n",
    "                local_batch[key] = value.to(device)\n",
    "                \n",
    "            #pred = model(local_batch['input'].unsqueeze(1))\n",
    "            y = local_batch['target']\n",
    "            #pred = model(y.unsqueeze(1))\n",
    "            \n",
    "            in_val = local_batch['target'].unsqueeze(1)\n",
    "            in_val_norm = normalization(in_val)\n",
    "            in_val_rand = rand_input(in_val_norm)\n",
    "            pred = model(in_val_rand)\n",
    "            \n",
    "            \n",
    "            #WEIGHTS\n",
    "            weight = calculate_weights(y)\n",
    "            class_weight = torch.FloatTensor(weight).to(device)\n",
    "            loss_fn = nn.CrossEntropyLoss(class_weight) \n",
    "            \n",
    "            test_loss += loss_fn(pred, y.long())#.detach()\n",
    "            \n",
    "            correct += (pred.argmax(axis=1) == y).type(torch.float).sum()\n",
    "            #correct += (pred.argmax(1) == y).type(torch.float).sum()#.detach()\n",
    "            tot_pts = torch.numel(y)\n",
    "            \n",
    "\n",
    "        size = batch_num + 1\n",
    "        test_loss /= size\n",
    "        #print (\"Correct:\", correct.shape(), \"\\nTotal Size: \", size*tot_pts)\n",
    "        correct /= (size*tot_pts)\n",
    "        print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")\n",
    "        \n",
    "        if torch.cuda.torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fed45af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603b4268",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 4, #Number of samples in each batch\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0} #Number of process to generate batches in //\n",
    "max_epochs = 10\n",
    "learning_rate = 1e-3\n",
    "num = 0 #FOR RANDOM SEED\n",
    "\n",
    "#Init\n",
    "output_array = []\n",
    "orig_array = []\n",
    "desc = \"First saving run. Only trained on the mask input\"\n",
    "\n",
    "#Training\n",
    "datafile = \"/nsls2/users/maire1/sim_data/sim_address.csv\"\n",
    "\n",
    "#def main(datafile, params, max_epochs):\n",
    "    \n",
    "# Determine and setup device\n",
    "device = cuda_setup()\n",
    "\n",
    "# Datasets\n",
    "file = pd.read_csv(datafile)\n",
    "partition = file['address']# IDs\n",
    "training_partition = partition[:800]\n",
    "validation_partition = partition[800:]\n",
    "\n",
    "labels = file['sample']# Labels\n",
    "training_labels = labels[:800]\n",
    "validation_labels = labels[800:]\n",
    "\n",
    "#model = UNet().to(device) #comment this out when you start training\n",
    "\n",
    "# Generators\n",
    "training_set = Dataset(training_partition, training_labels)\n",
    "training_generator = DataLoader(training_set, **params, pin_memory=True)\n",
    "\n",
    "validation_set = Dataset(validation_partition, validation_labels)\n",
    "validation_generator = DataLoader(validation_set, **params, pin_memory=True)\n",
    "# images, masks = next(iter(training_generator))['input'], next(iter(training_generator))['target']\n",
    "\n",
    "\n",
    "# weight = calculate_weights()\n",
    "# class_weight = torch.FloatTensor(weight).to(device)\n",
    "# loss_fn = nn.CrossEntropyLoss(class_weight)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# Loop over epochs\n",
    "for epoch in range(max_epochs):\n",
    "    # Training\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    output_array, orig_array, loss, optimizer = train(training_generator, model, optimizer, device, training_graphs=output_array, orig_images=orig_array)\n",
    "    validation(validation_generator, model, device)\n",
    "    \n",
    "#i = save_params(output_array, orig_array, params, desc, loss, model, optimizer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83701fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUNNING CELL\n",
    "#------------------------------------------------\n",
    "\n",
    "# Parameters\n",
    "params = {'batch_size': 2, #Number of samples in each batch\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0} #Number of process to generate batches in //\n",
    "max_epochs = 4\n",
    "\n",
    "#Training\n",
    "datafile = \"/nsls2/users/maire1/sim_data/sim_address.csv\"\n",
    "\n",
    "main(datafile, params, max_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf44457",
   "metadata": {},
   "source": [
    "#### Messing around with the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876cf812",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 1, #Number of samples in each batch\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0} #Number of process to generate batches in //\n",
    "max_epochs = 4\n",
    "\n",
    "#Training\n",
    "datafile = \"/nsls2/users/maire1/sim_data/sim_address.csv\"\n",
    "\n",
    "#def main(datafile, params, max_epochs):\n",
    "    \n",
    "# Determine and setup device\n",
    "device = cuda_setup()\n",
    "\n",
    "# Datasets\n",
    "file = pd.read_csv(datafile)\n",
    "partition = file['address']# IDs\n",
    "training_partition = partition[:80]\n",
    "validation_partition = partition[80:]\n",
    "\n",
    "labels = file['sample']# Labels\n",
    "training_labels = labels[:80]\n",
    "validation_labels = labels[80:]\n",
    "\n",
    "model = UNet().to(device)\n",
    "\n",
    "arr  = torch.rand([2,1,256,256]).float().to(device)\n",
    "model(arr)\n",
    "\n",
    "# size = len(dataloader)\n",
    "\n",
    "# #for batch_num, local_batch in enumerate (dataloader):\n",
    "    \n",
    "#     # Transfer to GPU\n",
    "#     for key, value in local_batch.items():\n",
    "#         local_batch[key] = value.to(device)\n",
    "\n",
    "#     in_batch = local_batch['input'].unsqueeze(1)\n",
    "#     print(in_batch.shape)\n",
    "#     out  = model(in_batch)\n",
    "\n",
    "#     #flatten target\n",
    "#     target = local_batch['target']\n",
    "#     loss = loss_fn(out, target.long()).cpu().detach()\n",
    "\n",
    "#     # Backpropagation\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     #if batch_num % 100 == 0:\n",
    "#     loss, current = loss.item(), batch_num #* len(out)\n",
    "#     print(f\"loss: {loss:>7f}  [{(current+1):>5d}/{size:>5d}]\")\n",
    "\n",
    "#     del in_batch, out, local_batch, target, loss, current, size, batch\n",
    "#     if torch.cuda.torch.cuda.is_available():\n",
    "#         torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b830a857",
   "metadata": {},
   "outputs": [],
   "source": [
    "del in_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2b19dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_batch = next(iter(training_generator))\n",
    "\n",
    "\n",
    "for key, value in local_batch.items():\n",
    "    local_batch[key] = value.to(device)\n",
    "        \n",
    "in_batch = local_batch['input'].unsqueeze(1)\n",
    "out  = model(in_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594d06cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUNNING CELL\n",
    "#------------------------------------------------\n",
    "\n",
    "# Parameters\n",
    "params = {'batch_size': 2, #Number of samples in each batch\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0} #Number of process to generate batches in //\n",
    "max_epochs = 4\n",
    "\n",
    "#Training\n",
    "datafile = \"/nsls2/users/maire1/sim_data/sim_address.csv\"\n",
    "\n",
    "main(datafile, params, max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91e2706",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda0c97d",
   "metadata": {},
   "source": [
    "mean squared error loss. y as target (y-y^)^2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
